{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f877233-89d9-4288-8c34-748d860320c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-python keras tensorflow numpy matplotlib tqdm pandas scikit-learn gradio transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b94c759-a919-47cc-b60f-bb1fa2fd69ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c644145-c420-4007-8c1a-1d90e8b9b051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Should print version number\n",
    "print(torch.cuda.is_available())  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2d75ce-a162-468e-9746-639c1fc707d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow is using GPU!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Force TensorFlow to use GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"✅ TensorFlow is using GPU!\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aad488f-4e6c-44ac-b5b3-4ab024d968e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23708/23708 [00:35<00:00, 672.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Loaded:  (18966, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Dataset Path\n",
    "DATASET_PATH = \"UTKFace/\"\n",
    "IMG_SIZE = 64  \n",
    "\n",
    "# Lists to store data\n",
    "ages, genders, images = [], [], []\n",
    "\n",
    "# Read images and labels\n",
    "for img_name in tqdm(os.listdir(DATASET_PATH)):\n",
    "    if img_name.endswith(\".jpg\"):\n",
    "        try:\n",
    "            # Parse filename (format: age_gender_race.jpg)\n",
    "            age, gender, _ = img_name.split(\"_\")[:3]\n",
    "            age, gender = int(age), int(gender)  # 0 = Male, 1 = Female\n",
    "\n",
    "            # Read and preprocess image\n",
    "            img = cv2.imread(os.path.join(DATASET_PATH, img_name))\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            img = img / 255.0  # Normalize\n",
    "\n",
    "            # Store data\n",
    "            images.append(img)\n",
    "            ages.append(age)\n",
    "            genders.append(gender)\n",
    "        except Exception as e:\n",
    "            continue  # Skip corrupted files\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(images)\n",
    "y_age = np.array(ages)\n",
    "y_gender = to_categorical(np.array(genders), 2)  # Male=0, Female=1\n",
    "\n",
    "# Split into train & test sets\n",
    "X_train, X_test, y_age_train, y_age_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    X, y_age, y_gender, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Data Loaded: \", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e659d1a1-fbe5-4675-bb0a-be1b9f25bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "593/593 [==============================] - 10s 13ms/step - loss: 0.4489 - accuracy: 0.7895 - val_loss: 0.3368 - val_accuracy: 0.8555\n",
      "Epoch 2/30\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 0.3187 - accuracy: 0.8601 - val_loss: 0.3014 - val_accuracy: 0.8667\n",
      "Epoch 3/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.2887 - accuracy: 0.8745 - val_loss: 0.2740 - val_accuracy: 0.8842\n",
      "Epoch 4/30\n",
      "593/593 [==============================] - 7s 11ms/step - loss: 0.2629 - accuracy: 0.8862 - val_loss: 0.2617 - val_accuracy: 0.8865\n",
      "Epoch 5/30\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 0.2459 - accuracy: 0.8934 - val_loss: 0.2641 - val_accuracy: 0.8889\n",
      "Epoch 6/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.2295 - accuracy: 0.9028 - val_loss: 0.2570 - val_accuracy: 0.8872\n",
      "Epoch 7/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.2183 - accuracy: 0.9064 - val_loss: 0.2612 - val_accuracy: 0.8893\n",
      "Epoch 8/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.2013 - accuracy: 0.9143 - val_loss: 0.2466 - val_accuracy: 0.8965\n",
      "Epoch 9/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1928 - accuracy: 0.9178 - val_loss: 0.2630 - val_accuracy: 0.8916\n",
      "Epoch 10/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1818 - accuracy: 0.9267 - val_loss: 0.2620 - val_accuracy: 0.8920\n",
      "Epoch 11/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1652 - accuracy: 0.9311 - val_loss: 0.2695 - val_accuracy: 0.8990\n",
      "Epoch 12/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1562 - accuracy: 0.9334 - val_loss: 0.2612 - val_accuracy: 0.8962\n",
      "Epoch 13/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.1444 - accuracy: 0.9406 - val_loss: 0.3155 - val_accuracy: 0.8954\n",
      "Epoch 14/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1367 - accuracy: 0.9409 - val_loss: 0.3085 - val_accuracy: 0.8973\n",
      "Epoch 15/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.1276 - accuracy: 0.9457 - val_loss: 0.3047 - val_accuracy: 0.8950\n",
      "Epoch 16/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1204 - accuracy: 0.9509 - val_loss: 0.3300 - val_accuracy: 0.8975\n",
      "Epoch 17/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.1105 - accuracy: 0.9554 - val_loss: 0.3485 - val_accuracy: 0.8973\n",
      "Epoch 18/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.1061 - accuracy: 0.9564 - val_loss: 0.3639 - val_accuracy: 0.8969\n",
      "Epoch 19/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0987 - accuracy: 0.9593 - val_loss: 0.3506 - val_accuracy: 0.8868\n",
      "Epoch 20/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.0894 - accuracy: 0.9631 - val_loss: 0.3810 - val_accuracy: 0.8998\n",
      "Epoch 21/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.0874 - accuracy: 0.9630 - val_loss: 0.3930 - val_accuracy: 0.8916\n",
      "Epoch 22/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0816 - accuracy: 0.9651 - val_loss: 0.4287 - val_accuracy: 0.8906\n",
      "Epoch 23/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0813 - accuracy: 0.9669 - val_loss: 0.4296 - val_accuracy: 0.8876\n",
      "Epoch 24/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0813 - accuracy: 0.9683 - val_loss: 0.4636 - val_accuracy: 0.8941\n",
      "Epoch 25/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0689 - accuracy: 0.9719 - val_loss: 0.5012 - val_accuracy: 0.8933\n",
      "Epoch 26/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0777 - accuracy: 0.9677 - val_loss: 0.4927 - val_accuracy: 0.8943\n",
      "Epoch 27/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0661 - accuracy: 0.9738 - val_loss: 0.4951 - val_accuracy: 0.8956\n",
      "Epoch 28/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.0671 - accuracy: 0.9731 - val_loss: 0.4811 - val_accuracy: 0.8916\n",
      "Epoch 29/30\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 0.0634 - accuracy: 0.9749 - val_loss: 0.5722 - val_accuracy: 0.8906\n",
      "Epoch 30/30\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 0.0688 - accuracy: 0.9735 - val_loss: 0.5021 - val_accuracy: 0.8884\n",
      "✅ Gender Model Saved!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# CNN Model for Gender Prediction (Classification)\n",
    "gender_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')  # 2 Classes: Male & Female\n",
    "])\n",
    "\n",
    "# Compile & Train Model\n",
    "gender_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "gender_model.fit(X_train, y_gender_train, epochs=30, batch_size=32, validation_data=(X_test, y_gender_test))\n",
    "\n",
    "# Save Model\n",
    "gender_model.save(\"gender_model.h5\")\n",
    "print(\"✅ Gender Model Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16bc9aaa-0360-4a6a-a471-15589aca324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "593/593 [==============================] - 9s 13ms/step - loss: 398.9722 - mae: 15.3243 - val_loss: 232.6627 - val_mae: 11.4557\n",
      "Epoch 2/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 239.7574 - mae: 11.7469 - val_loss: 177.3626 - val_mae: 9.8171\n",
      "Epoch 3/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 193.7592 - mae: 10.4770 - val_loss: 156.3341 - val_mae: 9.0719\n",
      "Epoch 4/40\n",
      "593/593 [==============================] - 7s 13ms/step - loss: 166.7695 - mae: 9.6265 - val_loss: 128.9050 - val_mae: 8.4908\n",
      "Epoch 5/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 151.5294 - mae: 9.1201 - val_loss: 119.5652 - val_mae: 7.8994\n",
      "Epoch 6/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 136.9631 - mae: 8.6476 - val_loss: 109.5841 - val_mae: 7.6410\n",
      "Epoch 7/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 131.5193 - mae: 8.4753 - val_loss: 107.3355 - val_mae: 7.5869\n",
      "Epoch 8/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 121.2386 - mae: 8.1343 - val_loss: 121.6659 - val_mae: 7.9889\n",
      "Epoch 9/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 115.2085 - mae: 7.9494 - val_loss: 99.3861 - val_mae: 7.2432\n",
      "Epoch 10/40\n",
      "593/593 [==============================] - 7s 13ms/step - loss: 109.7699 - mae: 7.7219 - val_loss: 95.5872 - val_mae: 7.1426\n",
      "Epoch 11/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 104.4594 - mae: 7.5361 - val_loss: 97.6662 - val_mae: 7.0755\n",
      "Epoch 12/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 102.9128 - mae: 7.4801 - val_loss: 95.5696 - val_mae: 7.1238\n",
      "Epoch 13/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 95.7779 - mae: 7.2253 - val_loss: 91.7833 - val_mae: 7.0680\n",
      "Epoch 14/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 94.7351 - mae: 7.1864 - val_loss: 91.4542 - val_mae: 6.9540\n",
      "Epoch 15/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 89.8676 - mae: 7.0375 - val_loss: 86.9068 - val_mae: 6.8063\n",
      "Epoch 16/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 85.6122 - mae: 6.8900 - val_loss: 94.8250 - val_mae: 6.9238\n",
      "Epoch 17/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 81.5756 - mae: 6.7226 - val_loss: 97.0111 - val_mae: 7.0466\n",
      "Epoch 18/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 79.8235 - mae: 6.6277 - val_loss: 92.3314 - val_mae: 6.9236\n",
      "Epoch 19/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 78.4596 - mae: 6.5544 - val_loss: 90.0343 - val_mae: 6.9518\n",
      "Epoch 20/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 75.0212 - mae: 6.4578 - val_loss: 112.5113 - val_mae: 7.6647\n",
      "Epoch 21/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 73.7815 - mae: 6.3979 - val_loss: 97.6878 - val_mae: 7.0725\n",
      "Epoch 22/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 69.1111 - mae: 6.2140 - val_loss: 101.7635 - val_mae: 7.1864\n",
      "Epoch 23/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 67.7590 - mae: 6.1574 - val_loss: 92.8883 - val_mae: 6.9694\n",
      "Epoch 24/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 65.3842 - mae: 6.0660 - val_loss: 90.7801 - val_mae: 6.8847\n",
      "Epoch 25/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 63.6022 - mae: 5.9674 - val_loss: 90.4429 - val_mae: 6.8661\n",
      "Epoch 26/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 62.6290 - mae: 5.9363 - val_loss: 91.8527 - val_mae: 6.9600\n",
      "Epoch 27/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 60.3212 - mae: 5.8240 - val_loss: 91.5379 - val_mae: 6.9915\n",
      "Epoch 28/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 59.6897 - mae: 5.8064 - val_loss: 93.8932 - val_mae: 7.0462\n",
      "Epoch 29/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 61.3878 - mae: 5.8403 - val_loss: 91.0677 - val_mae: 6.9776\n",
      "Epoch 30/40\n",
      "593/593 [==============================] - 8s 14ms/step - loss: 58.9275 - mae: 5.7691 - val_loss: 95.6432 - val_mae: 7.0377\n",
      "Epoch 31/40\n",
      "593/593 [==============================] - 9s 14ms/step - loss: 56.6869 - mae: 5.6588 - val_loss: 94.0974 - val_mae: 7.0032\n",
      "Epoch 32/40\n",
      "593/593 [==============================] - 8s 13ms/step - loss: 55.8359 - mae: 5.6462 - val_loss: 95.4166 - val_mae: 7.0757\n",
      "Epoch 33/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 55.4956 - mae: 5.6203 - val_loss: 93.5027 - val_mae: 7.1790\n",
      "Epoch 34/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 54.7438 - mae: 5.6011 - val_loss: 91.7548 - val_mae: 6.9829\n",
      "Epoch 35/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 54.0744 - mae: 5.5511 - val_loss: 97.1205 - val_mae: 7.1565\n",
      "Epoch 36/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 53.1271 - mae: 5.5281 - val_loss: 93.6490 - val_mae: 7.0469\n",
      "Epoch 37/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 51.2618 - mae: 5.4272 - val_loss: 91.9658 - val_mae: 7.0311\n",
      "Epoch 38/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 51.2471 - mae: 5.4324 - val_loss: 94.1892 - val_mae: 7.0733\n",
      "Epoch 39/40\n",
      "593/593 [==============================] - 7s 12ms/step - loss: 50.9131 - mae: 5.3958 - val_loss: 91.6952 - val_mae: 7.0356\n",
      "Epoch 40/40\n",
      "593/593 [==============================] - 7s 13ms/step - loss: 48.6194 - mae: 5.3116 - val_loss: 97.7459 - val_mae: 7.2156\n",
      "✅ Age Model Saved!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# CNN Model for Age Prediction (Regression)\n",
    "age_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')  # Regression output\n",
    "])\n",
    "\n",
    "# Compile & Train Model\n",
    "age_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "age_model.fit(X_train, y_age_train, epochs=40, batch_size=32, validation_data=(X_test, y_age_test),verbose=1)\n",
    "\n",
    "# Save Model\n",
    "age_model.save(\"age_model.h5\")\n",
    "print(\"✅ Age Model Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "745fb07d-60da-413b-842a-1e3fd26caa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# ✅ Set Device to GPU (if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using Device:\", device)\n",
    "\n",
    "# ✅ Load BLIP Model & Processor from Local Folder\n",
    "processor = BlipProcessor.from_pretrained(r\"C:\\Users\\OMEN\\Desktop\\python\\cmp\\Hugging\\blip\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(r\"C:\\Users\\OMEN\\Desktop\\python\\cmp\\Hugging\\blip\").to(device)\n",
    "\n",
    "# ✅ Load Age & Gender Detection Models\n",
    "age_model = load_model(\"age_model.h5\")      # Ensure these files exist\n",
    "gender_model = load_model(\"gender_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d1c13-94c0-46f5-8332-11f3cc31f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch transformers timm fairscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed35cc1-01cc-48eb-902e-148c79474b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load Trained Models\n",
    "age_model = load_model(\"age_model.h5\")\n",
    "gender_model = load_model(\"gender_model.h5\")\n",
    "\n",
    "print(\"✅ Models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc35f945-e7f6-4954-896b-066adc9873f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms  # <-- Add this line\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load BLIP from your local path\n",
    "blip_path = r\"C:\\Users\\OMEN\\Desktop\\python\\cmp\\Hugging\\blip\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(blip_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(blip_path).to(device)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def generate_description(image, predicted_age, predicted_gender):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Convert NumPy array to PIL Image if necessary\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    # Transform (Resize + Convert to Tensor)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize image to 224x224 for BLIP\n",
    "        transforms.ToTensor()  # Convert to tensor (No normalization)\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Convert back to PIL Image for BLIP\n",
    "    image_pil = transforms.ToPILImage()(image_tensor.squeeze(0))\n",
    "\n",
    "    # Ensure gender is a valid string\n",
    "    predicted_gender = \"Male\" if predicted_gender == 0 else \"Female\"\n",
    "\n",
    "    # Generate description using BLIP\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=image_pil, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs)\n",
    "\n",
    "    description = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Final formatted description\n",
    "    final_description = f\"{description} They appear to be a {predicted_gender} around {predicted_age} years old.\"\n",
    "    \n",
    "    return final_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91377a5-7b8b-4120-97b6-950f7c83b0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Ensure TensorFlow uses GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Load Trained Age & Gender Models on GPU\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    age_model = tf.keras.models.load_model(\"age_model.h5\")\n",
    "    gender_model = tf.keras.models.load_model(\"gender_model.h5\")\n",
    "\n",
    "# Function to Predict & Generate Descriptions\n",
    "def predict_and_describe(image):\n",
    "    if isinstance(image, dict):  # Ensure image is in NumPy format\n",
    "        image = image[\"image\"]\n",
    "\n",
    "    image = np.asarray(image, dtype=np.uint8)  # Convert to NumPy array\n",
    "    image_resized = cv2.resize(image, (64, 64)) / 255.0\n",
    "    image_resized = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "    # Predict Age & Gender on GPU\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        predicted_age = int(age_model.predict(image_resized)[0][0])\n",
    "        predicted_gender = np.argmax(gender_model.predict(image_resized))\n",
    "\n",
    "    # Generate Description using BLIP on GPU\n",
    "    with torch.autocast(\"cuda\"):  \n",
    "        description = generate_description(image, predicted_age, predicted_gender)\n",
    "\n",
    "    return f\"👤 Age: {predicted_age} | Gender: {'Male' if predicted_gender == 0 else 'Female'}\\n📜 Description: {description}\"\n",
    "\n",
    "# ✅ Use `gr.Interface()` Instead of `gr.Blocks()`\n",
    "interface = gr.Interface(\n",
    "    fn=predict_and_describe,\n",
    "    inputs=gr.Image(type=\"numpy\"),\n",
    "    outputs=gr.Textbox(label=\"Result\"),\n",
    "    title=\"Age & Gender Detection with BLIP Description\",\n",
    "    description=\"Upload an image to detect age, gender, and generate a description.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d0198-75ea-46ca-bfdf-391be82abea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f7c75-1638-4d9d-a937-1edc67009ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ae804-e3cb-424e-a18d-b7e1fd37f06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bd12d-eb3e-4cb5-b15f-ec740fc27b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env3)",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
